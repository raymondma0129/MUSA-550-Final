[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyzing Parking Trends in San Francisco, CA",
    "section": "",
    "text": "Samriddhi Khare, Roshini Ganesh Final Project, MUSA 550"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Analyzing Parking Trends in San Frncisco, CA",
    "section": "",
    "text": "San Francisco grapples with persistent challenges in its parking landscape, characterized by limited availability and high demand. The city’s densely populated neighborhoods often experience a shortage of parking spaces. These shortges are also reflective of certain social and demographic patterns in the city, as we have uncovered in this research.\nIn response to these difficulties, the use of parking apps has gained prominence, offering real-time information on available spaces and facilitating a more efficient navigation of the city’s intricate parking network. Increasing the efficiency of such apps could be a significant use case of this analysis.\nMoreover, the city has embraced innovative approaches to address parking issues. The implementation of smart parking meters, capable of adjusting rates based on demand and time of day, represents one such effort. Data from these parking meters has been included in our analysis. Additionally, there has been a push towards dynamic pricing strategies to incentivize turnover and optimize the utilization of parking spaces. These strategies can be more equitably informed if they take in to account how these policies are place within the larger sociodemographic framework of the city. As San Francisco continues to grapple with the complexities of urban parking, these trends underscore a broader shift towards sustainable transportation options and technological solutions in managing the city’s parking challenges."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "Analyzing Parking Trends in San Frncisco, CA",
    "section": "Find out more",
    "text": "Find out more\nThis project aims to utilize San Francisco’s open parking data to map and visualize parking availability, occupancy, and clustering trends within the city over the recent months/years. We utilized data from numerous sources to make inferences about parking trends in the city. We included data from:\n\nOpen parking dataset with locations\nParking meter data to cross-validate areas of high parking activity by recorded transactions\nOn-Street Parking census, which provides counts of publicly available, on-street parking for each street segment\nCensus data (using the API) for the selected geography\nOSM Street Maps data for street network analysis\n\nThe code for this repository is hosted on our GitHub repository."
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Analyzing Parking Spaces in LA",
    "section": "",
    "text": "Yutong Jiang, Ray Ma\n\n\nFinal Project, MUSA 550\n\n\nFile setup and data collection\nThe first step of this analysis comprises the essential tasks of loading necessary packages, configuring different APIs for data collection, and managing global environment settings.\n\n\nCode\n# Import packages\n\nimport altair as alt\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport pandas as pd\n#import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\nimport requests\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nfrom folium import plugins\nfrom shapely.geometry import Point\nimport xyzservices\nimport osmnx as ox\nimport networkx as nx\nimport pygris\nimport cenpy\n\n\n\n%matplotlib inline\n\n# See lots of columns\npd.options.display.max_rows = 9999 \npd.options.display.max_colwidth = 200\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\nThis step involves gathering data on parking for 2023 and preliminary data cleaning for the large dataset. All geospatial datasets are set to a uniform coordinate reference system, and boundary shapefiles are primed for use in OSM street network API.\n\nmeters = pd.read_csv('/Users/bin/Downloads/LADOT_Metered_Parking_Inventory___Policies_20241222.csv')\nprint(meters.columns)\nmeters.head()\n\nIndex(['SpaceID', 'BlockFace', 'MeterType', 'RateType', 'RateRange',\n       'MeteredTimeLimit', 'LatLng'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nSpaceID\nBlockFace\nMeterType\nRateType\nRateRange\nMeteredTimeLimit\nLatLng\n\n\n\n\n0\nWW516\n650 HARVARD BLVD\nSingle-Space\nFLAT\n$1.00\n2HR\n(34.060385, -118.304103)\n\n\n1\nCB3034\n201 E 4TH ST\nSingle-Space\nTOD\n$1.00 - $6.00\n2HR\n(34.047109, -118.245841)\n\n\n2\nBH398\n1901 1ST ST\nSingle-Space\nFLAT\n$1.00\n1HR\n(34.045795, -118.21555)\n\n\n3\nUC8\n3701 N CAHUENGA BLVD\nSingle-Space\nFLAT\n$1.00\n1HR\n(34.136733, -118.363025)\n\n\n4\nCB2345\n1401 S SAN PEDRO ST\nSingle-Space\nTOD\n$0.50 - $1.00\n4HR\n(34.030958, -118.255362)\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport numpy as np\n\n# Suppress warnings for invalid operations\nnp.seterr(invalid=\"ignore\")\n\n# Load parking meter data\nmeters = pd.read_csv('/Users/bin/Downloads/LADOT_Metered_Parking_Inventory___Policies_20241222.csv')\n\n# Remove parentheses and split LatLng into separate Latitude and Longitude columns\nmeters['LatLng'] = meters['LatLng'].str.strip(\"()\")\nmeters[['LATITUDE', 'LONGITUDE']] = meters['LatLng'].str.split(', ', expand=True).astype(float)\n\n# Convert to GeoDataFrame\ngeometry = [Point(xy) for xy in zip(meters['LONGITUDE'], meters['LATITUDE'])]\nmeters = gpd.GeoDataFrame(meters, geometry=geometry)\nmeters.crs = 'EPSG:4326'\n\n# Reproject to Web Mercator\nmeters = meters.to_crs('EPSG:3857')\n\n# Check the first few rows of the GeoDataFrame\n# print(meters.head())\n\n\nParking meters in LA\nThe interactive map below visually represents the distribution of parking meters in San Francisco, showcasing distinct levels of aggregation. Notably, a concentrated area with high meter density emerges in the northeast region, coinciding with the presence of major tech company headquarters. However, drawing conclusive insights from the map alone is challenging. Considerations such as street network density become key determinants of parking availability. Therefore, it is essential to contextualize this data with factors like street networks and population variables. It is crucial to recognize that a high availability of parking does not necessarily indicate an absence of scarcity; demand may still surpass supply.\n\nimport folium\nfrom folium.plugins import FastMarkerCluster\nimport xyzservices.providers\n\n# Extract LATITUDE and LONGITUDE columns\ncoords = meters[[\"LATITUDE\", \"LONGITUDE\"]].values.tolist()  # Convert to list of (lat, lon)\n\n# Create a map centered on Los Angeles with light mode\nm = folium.Map(\n    location=[34.05, -118.25],  # Center on Los Angeles\n    zoom_start=12,\n    tiles=xyzservices.providers.CartoDB.Positron  # Light mode tiles\n)\n\n# Add a FastMarkerCluster with the meter locations\nFastMarkerCluster(data=coords).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nOpen Street Map Data\nTo streamline the workflow with this large dataset, relevant OSM data is refined by excluding highways, where parking is not allowed. This ensures the dataset focuses solely on accessible areas with available parking spaces. A new graph is created and plotted to reflect only the non-highway streets.\n\nimport osmnx as ox\n\n# Define a smaller area or bounding box\ncity_name = 'Los Angeles, California, USA'\n\n# Retrieve the graph with simplification and largest component only\nG = ox.graph.graph_from_place(city_name, network_type='drive', simplify=True, retain_all=False)\n\n# Optional: Plot the graph\nox.plot_graph(G, bgcolor='k', node_color='w', node_size=5, edge_color='w', edge_linewidth=0.5)\n\n\n\n\n\n\n\n\n\n# Filter out highways (e.g., motorways, trunk roads)\nnon_highway_edges = [(u, v, key) for u, v, key, data in G.edges(keys=True, data=True) if 'highway' not in data or 'highway' in data and data['highway'] != 'motorway']\n\n# Create a new graph with non-highway streets\nG = G.edge_subgraph(non_highway_edges)\n\n# Plot the non-highway street network\n#ox.plot_graph(G, bgcolor='k', node_color='w', edge_color='w', node_size=5, edge_linewidth=0.5)\n\n\n# Step 1: Convert the graph to a GeoDataFrame containing edges\nla_edges = ox.graph_to_gdfs(G, edges=True, nodes=False)\n\n# Step 2: Project the graph to the EPSG:3857 coordinate reference system\nG_projected = ox.project_graph(G, to_crs='EPSG:3857')\n\n# Step 3: Extract longitude and latitude coordinates from the 'meters' GeoDataFrame\nx_coords = meters['geometry'].x  # Longitude in meters\ny_coords = meters['geometry'].y  # Latitude in meters\n\n# Step 4: Find the nearest edges for each parking meter\nnearest_edges = ox.distance.nearest_edges(G_projected, X=x_coords, Y=y_coords)\n\n# Step 5: Create a DataFrame with edge identifiers and count occurrences\nmeters_nodes = pd.DataFrame(nearest_edges, columns=['u', 'v', 'key'])\nmeters_nodes['Count'] = 1\n\n# Step 6: Group by edge identifiers and calculate total counts\ngrouped_counts = meters_nodes.groupby(['u', 'v'])['Count'].sum().reset_index()\n\n# Step 7: Merge edge counts with the edges GeoDataFrame\nmerged_gdf = la_edges.merge(grouped_counts, on=['u', 'v'], how='left')\n\n# Step 8: Filter rows with non-zero counts\nmerged_gdf = merged_gdf[merged_gdf['Count'] &gt; 0]\n\n# Step 9: Drop unnecessary columns for cleaner data\ncolumns_to_remove = [\n    'u', 'v', 'osmid', 'oneway', 'lanes', 'ref', 'maxspeed', \n    'reversed', 'access', 'bridge', 'junction', 'width', 'tunnel'\n]\nmerged_gdf = merged_gdf.drop(columns=columns_to_remove)\n\n# Step 10: Calculate the normalized count ('truecount')\nmerged_gdf['truecount'] = merged_gdf['Count'] / merged_gdf['length']\n\n# Step 11: Filter out edges with lengths outside the range [10, 100]\nlength_filter = (merged_gdf['length'] &gt;= 10) & (merged_gdf['length'] &lt;= 100)\nmerged_gdf = merged_gdf[length_filter]\n\n\nmerged_gdf.explore(tiles='cartodbdark_matter', column = 'truecount')\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWith this joined dataset as the base, the following transformations are performed:\n\nFirst, the latitude and logitude attributes for the cleaned OSM data are defined in xy coordinates to allow calculations related to location.\nNext, the nearest_edges function is used to determine the closest street edge to each parking meter.\nThird, the city’s parking meter data is integrated with the OSM Street Network data using the merge function to associate parking-related details with their corresponding street locations and the larger surrounding road infrastructure.\nThe joined dataset is then cleaned to:\n\nDrop columns that do not contribute to this study,\nEliminate streets that have zero parking meters, and\nRemove outlier values to only retain street lengths between 0 and 100 meters. This is done to ensure that the count of meters per street is normalized across the dataset. This is important because a 5-mile street segment could inherently accommodate more meters than a 1-mile street segment. The constraints distill the original dataset into a comparable dataset of street & parking factors.\n\n\n\nimport pandas as pd\nimport folium\nfrom folium.plugins import MarkerCluster\nimport xyzservices.providers\n\n# Example data with RateRange added\ndata = {\n    \"SpaceID\": [\"WW516\", \"CB3034\", \"BH398\"],\n    \"MeteredTimeLimit\": [\"2HR\", \"2HR\", \"1HR\"],\n    \"RateRange\": [\"$1.00\", \"$1.00 - $6.00\", \"$1.00\"],\n    \"LATITUDE\": [34.060385, 34.047109, 34.045795],\n    \"LONGITUDE\": [-118.304103, -118.245841, -118.21555],\n}\n\n# Convert to DataFrame\nmeters = pd.DataFrame(data)\n\n# Convert MeteredTimeLimit to numeric hours (e.g., \"2HR\" -&gt; 2)\nmeters['TimeLimit'] = meters['MeteredTimeLimit'].str.extract('(\\d+)').astype(float)\n\n# Convert RateRange to numeric (use midpoint for ranges)\ndef extract_rate(rate_range):\n    rates = [float(r.replace(\"$\", \"\")) for r in rate_range.split(\" - \")]\n    return sum(rates) / len(rates)  # Use midpoint\n\nmeters['RateValue'] = meters['RateRange'].apply(extract_rate)\n\n# Create a map centered on Los Angeles with light mode\nm = folium.Map(\n    location=[34.05, -118.25],  # Center on Los Angeles\n    zoom_start=12,\n    tiles=xyzservices.providers.CartoDB.Positron  # Light mode tiles\n)\n\n# Add CircleMarkers to the map with size based on RateValue\nfor _, row in meters.iterrows():\n    folium.CircleMarker(\n        location=[row['LATITUDE'], row['LONGITUDE']],\n        radius=row['RateValue'] * 5,  # Scale the size\n        color=\"blue\",\n        fill=True,\n        fill_opacity=0.6,\n        popup=f\"Metered Time Limit: {row['MeteredTimeLimit']}&lt;br&gt;Rate Range: {row['RateRange']}&lt;br&gt;Rate Value: ${row['RateValue']:.2f}\"\n    ).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# parking meters heated map\nimport folium\nfrom folium.plugins import HeatMap\nimport pandas as pd\n\n# Sample data as a DataFrame (replace with your actual data)\ndata = {\n    \"SpaceID\": [\"WW516\", \"CB3034\", \"BH398\", \"UC8\", \"CB2345\"],\n    \"LATITUDE\": [34.060385, 34.047109, 34.045795, 34.136733, 34.030958],\n    \"LONGITUDE\": [-118.304103, -118.245841, -118.215550, -118.363025, -118.255362],\n}\nmeters = pd.DataFrame(data)\n\n# Extract coordinates for the heatmap\ncoordinates = meters[[\"LATITUDE\", \"LONGITUDE\"]].values.tolist()\n\n# Create a map centered on Los Angeles with a light mode basemap\nm = folium.Map(location=[34.05, -118.25], zoom_start=12, tiles=\"CartoDB Positron\")\n\n# Add heatmap layer\nHeatMap(coordinates).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nimport pandas as pd\n\n# API URL for the CSV data\nurl = \"https://data.lacity.org/resource/2nrs-mtv8.csv\"\n\n# Read the CSV file from the API\ncrime_data = pd.read_csv(url)\n\ncrime_data.head()\n\n\n\n\n\n\n\n\ndr_no\ndate_rptd\ndate_occ\ntime_occ\narea\narea_name\nrpt_dist_no\npart_1_2\ncrm_cd\ncrm_cd_desc\n...\nstatus\nstatus_desc\ncrm_cd_1\ncrm_cd_2\ncrm_cd_3\ncrm_cd_4\nlocation\ncross_street\nlat\nlon\n\n\n\n\n0\n190326475\n2020-03-01T00:00:00.000\n2020-03-01T00:00:00.000\n2130\n7\nWilshire\n784\n1\n510\nVEHICLE - STOLEN\n...\nAA\nAdult Arrest\n510\n998.0\nNaN\nNaN\n1900 S LONGWOOD AV\nNaN\n34.0375\n-118.3506\n\n\n1\n200106753\n2020-02-09T00:00:00.000\n2020-02-08T00:00:00.000\n1800\n1\nCentral\n182\n1\n330\nBURGLARY FROM VEHICLE\n...\nIC\nInvest Cont\n330\n998.0\nNaN\nNaN\n1000 S FLOWER ST\nNaN\n34.0444\n-118.2628\n\n\n2\n200320258\n2020-11-11T00:00:00.000\n2020-11-04T00:00:00.000\n1700\n3\nSouthwest\n356\n1\n480\nBIKE - STOLEN\n...\nIC\nInvest Cont\n480\nNaN\nNaN\nNaN\n1400 W 37TH ST\nNaN\n34.0210\n-118.3002\n\n\n3\n200907217\n2023-05-10T00:00:00.000\n2020-03-10T00:00:00.000\n2037\n9\nVan Nuys\n964\n1\n343\nSHOPLIFTING-GRAND THEFT ($950.01 & OVER)\n...\nIC\nInvest Cont\n343\nNaN\nNaN\nNaN\n14000 RIVERSIDE DR\nNaN\n34.1576\n-118.4387\n\n\n4\n200412582\n2020-09-09T00:00:00.000\n2020-09-09T00:00:00.000\n630\n4\nHollenbeck\n413\n1\n510\nVEHICLE - STOLEN\n...\nIC\nInvest Cont\n510\nNaN\nNaN\nNaN\n200 E AVENUE 28\nNaN\n34.0820\n-118.2130\n\n\n\n\n5 rows × 28 columns\n\n\n\n\n# vehicle related crime map\nimport pandas as pd\nimport folium\n\n# API URL for the CSV data\nurl = \"https://data.lacity.org/resource/2nrs-mtv8.csv\"\n\n# Read the CSV file from the API\ncrime_data = pd.read_csv(url)\n\n# Filter for \"VEHICLE - STOLEN\" cases\nvehicle_stolen_data = crime_data[crime_data['crm_cd_desc'] == \"VEHICLE - STOLEN\"]\n\n# Check for valid lat/lon data and drop rows with missing coordinates\nvehicle_stolen_data = vehicle_stolen_data.dropna(subset=['lat', 'lon'])\n\n# Convert lat/lon to numeric (in case they are read as strings)\nvehicle_stolen_data['lat'] = pd.to_numeric(vehicle_stolen_data['lat'])\nvehicle_stolen_data['lon'] = pd.to_numeric(vehicle_stolen_data['lon'])\n\n# Create a map centered on Los Angeles with a light mode basemap\nm = folium.Map(location=[34.05, -118.25], zoom_start=12, tiles=\"CartoDB Positron\")\n\n# Add markers for each \"VEHICLE - STOLEN\" case\nfor _, row in vehicle_stolen_data.iterrows():\n    folium.Marker(\n        location=[row['lat'], row['lon']],\n        popup=f\"Location: {row['location']}&lt;br&gt;Date: {row['date_occ']}\"\n    ).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nimport pandas as pd\nimport folium\nfrom folium.plugins import HeatMap\n\n# API URL for the CSV data\nurl = \"https://data.lacity.org/resource/2nrs-mtv8.csv\"\n\n# Read the CSV file from the API\ncrime_data = pd.read_csv(url)\n\n# Filter for \"VEHICLE - STOLEN\" cases\nvehicle_stolen_data = crime_data[crime_data['crm_cd_desc'] == \"VEHICLE - STOLEN\"]\n\n# Drop rows with missing coordinates\nvehicle_stolen_data = vehicle_stolen_data.dropna(subset=['lat', 'lon'])\n\n# Convert lat/lon to numeric\nvehicle_stolen_data['lat'] = pd.to_numeric(vehicle_stolen_data['lat'])\nvehicle_stolen_data['lon'] = pd.to_numeric(vehicle_stolen_data['lon'])\n\n# Extract latitude and longitude as a list of [lat, lon]\nheat_data = vehicle_stolen_data[['lat', 'lon']].values.tolist()\n\n# Create a map centered on Los Angeles with a light mode basemap\nm = folium.Map(location=[34.05, -118.25], zoom_start=12, tiles=\"CartoDB Positron\")\n\n# Add the heatmap layer\nHeatMap(heat_data, radius=10).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# Import packages\n\nimport altair as alt\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport pandas as pd\n#import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\nimport requests\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nfrom folium import plugins\nfrom shapely.geometry import Point\nimport xyzservices\nimport osmnx as ox\nimport networkx as nx\nimport pygris\nimport cenpy\n\n\n\n%matplotlib inline\n\n# See lots of columns\npd.options.display.max_rows = 9999 \npd.options.display.max_colwidth = 200\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\navailable = cenpy.explorer.available()\navailable.head()\n\n# Return a dataframe of\n\n/Users/bin/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/cenpy/explorer.py:70: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  listcols = raw_table.applymap(lambda x: isinstance(x, list)).any()\n\n\n\n\n\n\n\n\n\nc_isTimeseries\nc_isMicrodata\npublisher\ntemporal\nspatial\nprogramCode\nmodified\nkeyword\ncontactPoint\ndistribution\ndescription\nbureauCode\naccessLevel\ntitle\nc_isAvailable\nc_isCube\nc_isAggregate\nc_dataset\nvintage\n\n\n\n\nABSCB2017\nNaN\nNaN\nU.S. Census Bureau\n2017/2017\nUnited States\n006:007\n2020-04-30 00:00:00.0\n(census,)\n{'fn': 'ABS Staff', 'hasEmail': 'mailto:adep.annual.business.survey@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2017/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, ...\n006:07\npublic\nAnnual Business Survey: Characteristics of Businesses: 2017\nTrue\nNaN\nTrue\n(abscb,)\n2017.0\n\n\nABSCB2018\nNaN\nNaN\nU.S. Census Bureau\n2018/2018\nUnited States\n006:007\n2020-10-26 00:00:00.0\n(census,)\n{'fn': 'ABS Staff', 'hasEmail': 'mailto:adep.annual.business.survey@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2018/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, ...\n006:07\npublic\nAnnual Business Survey: Characteristics of Businesses: 2018\nTrue\nNaN\nTrue\n(abscb,)\n2018.0\n\n\nABSCB2019\nNaN\nNaN\nU.S. Census Bureau\n2019/2019\nUS\n006:007\n2021-08-17 00:00:00.0\n(census,)\n{'fn': 'ASE Staff', 'hasEmail': 'mailto:ERD.annual.survey.of.entrepreneurs@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2019/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, ...\n006:07\npublic\n2019 Annual Business Survey: Characteristics of Business\nTrue\nNaN\nTrue\n(abscb,)\n2019.0\n\n\nABSCB2020\nNaN\nNaN\nU.S. Census Bureau\n2020/2020\nUS\n006:007\n2022-08-03 00:00:00.0\n(census,)\n{'fn': 'ASE Staff', 'hasEmail': 'mailto:ERD.annual.survey.of.entrepreneurs@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2020/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, ...\n006:07\npublic\n2020 Annual Business Survey: Characteristics of Business\nTrue\nNaN\nTrue\n(abscb,)\n2020.0\n\n\nABSCB2021\nNaN\nNaN\nU.S. Census Bureau\n2021/2021\nUnited States\n006:007\n2023-07-24 10:30:52.0\n(census,)\n{'fn': 'ABS Staff', 'hasEmail': 'mailto:adep.annual.business.survey@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2021/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, ...\n006:07\npublic\n2021 Annual Business Survey: Characteristics of Business\nTrue\nNaN\nTrue\n(abscb,)\n2021.0\n\n\n\n\n\n\n\n\n# Connect to Census API\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\n\n# Define variables of interest\nvariables = [\n    \"NAME\",\n    \"B19013_001E\",  # Median income\n    \"B03002_001E\",  # Total population\n    \"B03002_003E\",  # Not Hispanic, White\n    \"B03002_012E\",  # Hispanic or Latino\n    \"B08301_001E\",  # Means of transportation to work\n    \"B08301_010E\"   # Public transportation\n]\n\n# Los Angeles County and California codes\nla_county_code = \"037\"\nca_state_code = \"06\"\n\n# Query ACS data for Los Angeles block groups\nla_inc_data = acs.query(\n    cols=variables,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": ca_state_code, \"county\": la_county_code, \"tract\": \"*\"}\n)\n\n# Convert numerical columns to float\nfor variable in variables:\n    if variable != \"NAME\":\n        la_inc_data[variable] = la_inc_data[variable].astype(float)\n\n\n# Create a cleaned DataFrame from la_inc_data\nla_final = la_inc_data.copy()\n\n# Adjusted columns to drop\ncolumns_to_drop = [\n    \"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\", \"GEOID\", \"NAMELSAD\",\n    \"MTFCC\", \"FUNCSTAT\", \"ALAND\", \"AWATER\", \"INTPTLAT\", \"INTPTLON\"\n]\n\n# Drop unnecessary columns\nif all(col in la_final.columns for col in columns_to_drop):\n    la_final.drop(columns=columns_to_drop, inplace=True)\nelse:\n    missing_cols = [col for col in columns_to_drop if col not in la_final.columns]\n    print(f\"Warning: The following columns are missing and cannot be dropped: {missing_cols}\")\n\n# Verify the structure of the cleaned DataFrame\nprint(la_final.columns)\nla_final.head()\n\nWarning: The following columns are missing and cannot be dropped: ['STATEFP', 'COUNTYFP', 'TRACTCE', 'BLKGRPCE', 'GEOID', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'ALAND', 'AWATER', 'INTPTLAT', 'INTPTLON']\nIndex(['NAME', 'B19013_001E', 'B03002_001E', 'B03002_003E', 'B03002_012E',\n       'B08301_001E', 'B08301_010E', 'state', 'county', 'tract',\n       'block group'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nNAME\nB19013_001E\nB03002_001E\nB03002_003E\nB03002_012E\nB08301_001E\nB08301_010E\nstate\ncounty\ntract\nblock group\n\n\n\n\n0\nBlock Group 1, Census Tract 1011.10, Los Angeles County, California\n63242.0\n1630.0\n932.0\n571.0\n697.0\n13.0\n06\n037\n101110\n1\n\n\n1\nBlock Group 2, Census Tract 1011.10, Los Angeles County, California\n56250.0\n1492.0\n864.0\n314.0\n772.0\n47.0\n06\n037\n101110\n2\n\n\n2\nBlock Group 3, Census Tract 1011.10, Los Angeles County, California\n99567.0\n757.0\n509.0\n120.0\n468.0\n0.0\n06\n037\n101110\n3\n\n\n3\nBlock Group 1, Census Tract 1011.22, Los Angeles County, California\n120833.0\n2608.0\n1879.0\n117.0\n1195.0\n0.0\n06\n037\n101122\n1\n\n\n4\nBlock Group 2, Census Tract 1011.22, Los Angeles County, California\n90536.0\n1639.0\n1061.0\n222.0\n782.0\n9.0\n06\n037\n101122\n2\n\n\n\n\n\n\n\n\n# Rename columns for easier interpretation\nla_final.rename(columns={\n    \"B19013_001E\": \"Median Income\",\n    \"B03002_001E\": \"Total Population\",\n    \"B03002_003E\": \"White Population\",\n    \"B03002_012E\": \"Hispanic Population\",\n    \"B08301_001E\": \"Total Commuters\",\n    \"B08301_010E\": \"Public Transit Commuters\"\n}, inplace=True)\n\n\nimport geopandas as gpd\nimport pygris\n\n# Step 1: Import block group geometries\nblock_groups = pygris.block_groups(state=\"CA\", county=\"037\", year=2021)\n\n# Step 2: Rename columns in block_groups to match la_final\nblock_groups.rename(\n    columns={\n        \"STATEFP\": \"state\",\n        \"COUNTYFP\": \"county\",\n        \"TRACTCE\": \"tract\",\n        \"BLKGRPCE\": \"block group\"\n    },\n    inplace=True\n)\n\n# Step 3: Merge with your dataset\nla_final_geo = block_groups.merge(la_final, on=[\"state\", \"county\", \"tract\", \"block group\"], how=\"left\")\n\n# Step 4: Set the CRS\nla_final_geo.crs = \"EPSG:4326\"\n\n# Step 5: Visualize Median Income\nla_final_geo.explore(\n    column=\"Median Income\",\n    tiles=\"cartodbdark_matter\",\n    legend=True\n)\n\nUsing FIPS code '06' for input 'CA'\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Check if the necessary columns exist\nif 'lat' in crime_data.columns and 'lon' in crime_data.columns:\n    # Create geometry from latitude and longitude\n    crime_data['geometry'] = gpd.points_from_xy(crime_data['lon'], crime_data['lat'])\n    \n    # Convert to GeoDataFrame\n    crime_gdf = gpd.GeoDataFrame(crime_data, geometry='geometry', crs=\"EPSG:4326\")\n    \n    # Check the first few rows to confirm\n    print(crime_gdf.head())\nelse:\n    print(\"Latitude ('lat') or Longitude ('lon') columns are missing in the dataset.\")\n\n       dr_no                date_rptd                 date_occ  time_occ  \\\n0  190326475  2020-03-01T00:00:00.000  2020-03-01T00:00:00.000      2130   \n1  200106753  2020-02-09T00:00:00.000  2020-02-08T00:00:00.000      1800   \n2  200320258  2020-11-11T00:00:00.000  2020-11-04T00:00:00.000      1700   \n3  200907217  2023-05-10T00:00:00.000  2020-03-10T00:00:00.000      2037   \n4  200412582  2020-09-09T00:00:00.000  2020-09-09T00:00:00.000       630   \n\n   area   area_name  rpt_dist_no  part_1_2  crm_cd  \\\n0     7    Wilshire          784         1     510   \n1     1     Central          182         1     330   \n2     3   Southwest          356         1     480   \n3     9    Van Nuys          964         1     343   \n4     4  Hollenbeck          413         1     510   \n\n                                crm_cd_desc  ...   status_desc  crm_cd_1  \\\n0                          VEHICLE - STOLEN  ...  Adult Arrest       510   \n1                     BURGLARY FROM VEHICLE  ...   Invest Cont       330   \n2                             BIKE - STOLEN  ...   Invest Cont       480   \n3  SHOPLIFTING-GRAND THEFT ($950.01 & OVER)  ...   Invest Cont       343   \n4                          VEHICLE - STOLEN  ...   Invest Cont       510   \n\n  crm_cd_2 crm_cd_3  crm_cd_4                                  location  \\\n0    998.0      NaN       NaN   1900 S  LONGWOOD                     AV   \n1    998.0      NaN       NaN   1000 S  FLOWER                       ST   \n2      NaN      NaN       NaN   1400 W  37TH                         ST   \n3      NaN      NaN       NaN  14000    RIVERSIDE                    DR   \n4      NaN      NaN       NaN                          200 E  AVENUE 28   \n\n   cross_street      lat       lon                     geometry  \n0           NaN  34.0375 -118.3506  POINT (-118.35060 34.03750)  \n1           NaN  34.0444 -118.2628  POINT (-118.26280 34.04440)  \n2           NaN  34.0210 -118.3002  POINT (-118.30020 34.02100)  \n3           NaN  34.1576 -118.4387  POINT (-118.43870 34.15760)  \n4           NaN  34.0820 -118.2130  POINT (-118.21300 34.08200)  \n\n[5 rows x 29 columns]\n\n\n\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\n# Define the LA bounding box\nla_bbox = gpd.GeoDataFrame(\n    {'geometry': [Polygon([\n        (-118.6682, 33.7045),  # Southwest corner\n        (-118.1553, 33.7045),  # Southeast corner\n        (-118.1553, 34.3373),  # Northeast corner\n        (-118.6682, 34.3373),  # Northwest corner\n        (-118.6682, 33.7045)   # Close polygon\n    ])]},\n    crs=\"EPSG:4326\"\n)\n\n# Ensure `meters` contains the necessary latitude and longitude data\nif 'LATITUDE' in meters.columns and 'LONGITUDE' in meters.columns:\n    # Create a GeoDataFrame for meters\n    meters['geometry'] = gpd.points_from_xy(meters['LONGITUDE'], meters['LATITUDE'])\n    meters_gdf = gpd.GeoDataFrame(meters, geometry='geometry', crs=\"EPSG:4326\")\nelse:\n    raise ValueError(\"Meters dataset must have 'LATITUDE' and 'LONGITUDE' columns.\")\n\n# Filter crime data within the bounding box\ncrime_gdf = crime_gdf[crime_gdf.intersects(la_bbox.unary_union)]\n\n# Filter meters data within the bounding box\nmeters_gdf = meters_gdf[meters_gdf.intersects(la_bbox.unary_union)]\n\n# Verify the results\nprint(\"Filtered Crime Data:\")\nprint(crime_gdf.head())\n\nprint(\"Filtered Meters Data:\")\nprint(meters_gdf.head())\n\nFiltered Crime Data:\n       dr_no                date_rptd                 date_occ  time_occ  \\\n0  190326475  2020-03-01T00:00:00.000  2020-03-01T00:00:00.000      2130   \n1  200106753  2020-02-09T00:00:00.000  2020-02-08T00:00:00.000      1800   \n2  200320258  2020-11-11T00:00:00.000  2020-11-04T00:00:00.000      1700   \n3  200907217  2023-05-10T00:00:00.000  2020-03-10T00:00:00.000      2037   \n4  200412582  2020-09-09T00:00:00.000  2020-09-09T00:00:00.000       630   \n\n   area   area_name  rpt_dist_no  part_1_2  crm_cd  \\\n0     7    Wilshire          784         1     510   \n1     1     Central          182         1     330   \n2     3   Southwest          356         1     480   \n3     9    Van Nuys          964         1     343   \n4     4  Hollenbeck          413         1     510   \n\n                                crm_cd_desc  ...   status_desc  crm_cd_1  \\\n0                          VEHICLE - STOLEN  ...  Adult Arrest       510   \n1                     BURGLARY FROM VEHICLE  ...   Invest Cont       330   \n2                             BIKE - STOLEN  ...   Invest Cont       480   \n3  SHOPLIFTING-GRAND THEFT ($950.01 & OVER)  ...   Invest Cont       343   \n4                          VEHICLE - STOLEN  ...   Invest Cont       510   \n\n  crm_cd_2 crm_cd_3  crm_cd_4                                  location  \\\n0    998.0      NaN       NaN   1900 S  LONGWOOD                     AV   \n1    998.0      NaN       NaN   1000 S  FLOWER                       ST   \n2      NaN      NaN       NaN   1400 W  37TH                         ST   \n3      NaN      NaN       NaN  14000    RIVERSIDE                    DR   \n4      NaN      NaN       NaN                          200 E  AVENUE 28   \n\n   cross_street      lat       lon                     geometry  \n0           NaN  34.0375 -118.3506  POINT (-118.35060 34.03750)  \n1           NaN  34.0444 -118.2628  POINT (-118.26280 34.04440)  \n2           NaN  34.0210 -118.3002  POINT (-118.30020 34.02100)  \n3           NaN  34.1576 -118.4387  POINT (-118.43870 34.15760)  \n4           NaN  34.0820 -118.2130  POINT (-118.21300 34.08200)  \n\n[5 rows x 29 columns]\nFiltered Meters Data:\n  SpaceID   LATITUDE   LONGITUDE                     geometry\n0   WW516  34.060385 -118.304103  POINT (-118.30410 34.06038)\n1  CB3034  34.047109 -118.245841  POINT (-118.24584 34.04711)\n2   BH398  34.045795 -118.215550  POINT (-118.21555 34.04579)\n3     UC8  34.136733 -118.363025  POINT (-118.36302 34.13673)\n4  CB2345  34.030958 -118.255362  POINT (-118.25536 34.03096)\n\n\n\n# Perform spatial join to associate crimes with grid cells\ngrid_with_crimes = gpd.sjoin(grid, crime_gdf, how=\"left\", op=\"intersects\")\n\n# Count crimes in each grid cell\ncrime_counts = grid_with_crimes.groupby(grid_with_crimes.index).size()\n\n# Add crime counts back to the grid\ngrid['crime_count'] = grid.index.map(crime_counts).fillna(0)\n\n/Users/bin/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n# Perform spatial join to associate parking meters with grid cells\ngrid_with_meters = gpd.sjoin(grid, meters_gdf, how=\"left\", op=\"intersects\")\n\n# Count parking meters in each grid cell\nmeter_counts = grid_with_meters.groupby(grid_with_meters.index).size()\n\n# Add meter counts back to the grid\ngrid['meter_count'] = grid.index.map(meter_counts).fillna(0)\n\n/Users/bin/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n# Normalize scores\ngrid['crime_score'] = grid['crime_count'] / grid['crime_count'].max()\ngrid['meter_score'] = grid['meter_count'] / grid['meter_count'].max()\n\n# Assign weights\ncrime_weight = 0.6\nmeter_weight = 0.4\n\n# Calculate composite score\ngrid['recommendation_score'] = grid['meter_score'] * meter_weight - grid['crime_score'] * crime_weight\n\n\n# Visualize the recommendation score\ngrid.explore(\n    column='recommendation_score',\n    cmap='RdYlGn',  # Green for high scores, red for low scores\n    legend=True,\n    legend_kwds={'caption': 'Parking Recommendation Score'}\n)\n\n# Save results\ngrid.to_file('./data/recommended_parking.geojson', driver='GeoJSON')\n\n\nprint(grid.columns)\ngrid = grid.reset_index()\nprint(grid.columns)  # Confirm 'index' is now a column\n\nIndex(['geometry', 'crime_count', 'meter_count', 'crime_score', 'meter_score',\n       'recommendation_score'],\n      dtype='object')\nIndex(['index', 'geometry', 'crime_count', 'meter_count', 'crime_score',\n       'meter_score', 'recommendation_score'],\n      dtype='object')\n\n\n\nimport folium\n\n# Initialize the folium map with light tiles, centered on Downtown LA\nm = folium.Map(location=[34.0407, -118.2468], zoom_start=14, tiles='cartodbpositron')\n\n# Add a Choropleth layer for recommendation scores\nfolium.Choropleth(\n    geo_data=grid,  # GeoDataFrame with grid and recommendation score\n    data=grid,  # Data for the Choropleth\n    columns=['index', 'recommendation_score'],  # Columns to use: 'index' and score\n    key_on='feature.properties.index',  # Match GeoJSON 'index' with data 'index'\n    fill_color='RdYlGn',  # Green for high scores, red for low scores\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    legend_name='Parking Recommendation Score'\n).add_to(m)\n\n# Display the map in the notebook\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Analyzing Parking Spaces in LA"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Analyzing Parking Trends in San Francisco, California",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Analyzing Parking Trends in San Francisco, California",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About us",
    "section": "",
    "text": "Samriddhi Khare (top) is currently a Research Fellow at Econsult Solutions’ Center for Future of Cities. In 2024, she received a Master’s in City Planning from the University of Pennsylvania, focusing on cities, data, and technology.\nRoshini Ganesh (bottom) is a City Planning candidate at UPenn interested in building connected, inclusive, and resilient smart cities using ethical technology. She currently works in transportation planning at Amtrak.\nTogether, we share an enthusiam for all things data and cities. This project was conceptualized during a professional visit we made to San Francisco in late 2023, where we observed first hand the city’s overt reliance on personal vehicles as the preferred mode of transportation and the equity and sustainabilty challenges that come with it.\nWe have created this project using Python, rendering on Quarto and published through Github pages. You can find more information about us on our personal websites linked above."
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nWe divided our analysis in to three distinct steps. Each sub-section highlights different types of analyses and visualizations:\n\nExploratory Analysis: Exploring the parking meter dataset to understand the relationships between the variables included.\nCensus API Data Collection: Collecting socio-economic variables from the American Community Survey fro the year 2021 and joining it to our street and meters data to explore patterns.\nSpatial Processes and Correlations: Exploring the relationships between variables after performing the join and observing correlations between calculated and chosed metrics."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Analyzing Parking Trends in San Francisco, CA",
    "section": "Introduction",
    "text": "Introduction\nSan Francisco grapples with persistent challenges in its parking landscape, characterized by limited availability and high demand. The city’s densely populated neighborhoods often experience a shortage of parking spaces. These shortges are also reflective of certain social and demographic patterns in the city, as we have uncovered in this research.\nIn response to these difficulties, the use of parking apps has gained prominence, offering real-time information on available spaces and facilitating a more efficient navigation of the city’s intricate parking network. Increasing the efficiency of such apps could be a significant use case of this analysis.\nMoreover, the city has embraced innovative approaches to address parking issues. The implementation of smart parking meters, capable of adjusting rates based on demand and time of day, represents one such effort. Data from these parking meters has been included in our analysis. Additionally, there has been a push towards dynamic pricing strategies to incentivize turnover and optimize the utilization of parking spaces. These strategies can be more equitably informed if they take in to account how these policies are place within the larger sociodemographic framework of the city. As San Francisco continues to grapple with the complexities of urban parking, these trends underscore a broader shift towards sustainable transportation options and technological solutions in managing the city’s parking challenges."
  },
  {
    "objectID": "index.html#data-sources",
    "href": "index.html#data-sources",
    "title": "Analyzing Parking Trends in San Francisco, CA",
    "section": "Data Sources",
    "text": "Data Sources\nThis project aims to utilize San Francisco’s open parking data to map and visualize parking availability, occupancy, and clustering trends within the city over the recent months/years. We utilized data from numerous sources to make inferences about parking trends in the city. We included data from:\n\nOpen parking dataset with locations\nParking meter data to cross-validate areas of high parking activity by recorded transactions\nOn-Street Parking census, which provides counts of publicly available, on-street parking for each street segment\nCensus data (using the API) for the selected geography\nOSM Street Maps data for street network analysis\n\nThe code for this repository is hosted on our GitHub repository."
  },
  {
    "objectID": "analysis/5-folium.html",
    "href": "analysis/5-folium.html",
    "title": "Spatial Processes and Correlations",
    "section": "",
    "text": "Spatial Join: Census OSM and Meter locations\nUpon hovering over areas of interest, the count of meters and associated variables, including wealth and demographic indicators, are observed. Preliminary findings suggest a correlation between areas with a high density of parking meters and indicators of affluence, such as higher socioeconomic status and a predominantly white population. Further analysis aims to uncover additional factors contributing to the observed patterns.\n\n\nCode\n# Import packages\n\nimport altair as alt\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\nimport requests\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nfrom folium import plugins\nfrom shapely.geometry import Point\nimport xyzservices\nimport osmnx as ox\nimport networkx as nx\nimport pygris\nimport cenpy\n\n\n\n%matplotlib inline\n\n# See lots of columns\npd.options.display.max_rows = 9999 \npd.options.display.max_colwidth = 200\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# joining census and OSM data \n\n#columns_to_drop = ['in']\n\n#sf_final.drop(columns=columns_to_drop, inplace=True)\nsf_final = gpd.read_file(\"./data/census.geojson\")\nmerged_gdf = gpd.read_file(\"./data/merged_gdf.geojson\")\n\nsf_final = sf_final.to_crs('EPSG:3857')\n\nmerged_gdf = merged_gdf.to_crs('EPSG:3857')\n\nfinal_gdf = gpd.sjoin(merged_gdf, sf_final, how='left', op='intersects')\n\n#columns_to_drop = ['STATEFP', 'COUNTYFP', 'TRACTCE', 'BLKGRPCE', 'GEOID', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'INTPTLAT', 'INTPTLON']\n\n#final_gdf.drop(columns=columns_to_drop, inplace=True)\n\n\n#final_gdf.head()\n\n\nD:\\Fall_2023\\Python\\Mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n\nCode\n#another map showing census data by street\n\nfinal_gdf.explore(column=\"Count\", tiles=\"cartodbdark_matter\")\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCorrelations\nTo understand distribution trends with more precision, a correlation test is conducted to identify the variables that exhibit the strongest statistical relationship with the count of parking meters. This information on influencing variables could inform predictive modeling for the future placement of parking meters.\n\n\nCode\n# correlations\n\n# The feature columns we want to use\nimport seaborn as sns\n\n\ncols = [\n    \"Count\",\n    \"truecount\",\n    \"length\",\n 'Total_Pop', \n    'Med_Inc', \n    'White_Pop', \n    'Travel_Time', \n    'Means_of_Transport', \n    'Total_Public_Trans', \n    'Med_Age', \n    'workforce_16', \n    'Num_Vehicles', \n    'households_NOcar_pct', \n    'households_car_pct', \n    'commute_30_90min_pct', \n    'Drive_Work_pct', \n    'PublicTransport_work_pct', \n    'Percent_White', \n    'Mean_Commute_Time', \n    'Percent_Taking_Public_Trans'\n]\n\n\n\n# Trim to these columns and remove NaNs\nfinal_corr = final_gdf[cols + [\"geometry\"]].dropna()\n#final_corr.head()\n\n\n\n\nCode\nplt.figure(figsize=(15, 15))\nsns.heatmap(final_corr[cols].corr(), cmap='coolwarm', annot=True, vmin=-1, vmax=1)\n\nplt.show()\n\n\n\n\n\nWe can see here that factors like percentage of people taking public transit and residents having longer commute times have a negative correlation with number of parking meters per street, indicating that an increase in these variables can be associated with meter-rich areas.\n\n\nConclusion and Next Steps\nThe patterns we have uncovered through this analysis not only sheds light on the current state of parking demand but also equips us with the predictive tools needed to anticipate future trends. Harnessing this knowledge can enable city officials and policymakers to proactively address the growing challenges of parking management, creating sustainable solutions that cater to the specific needs of different communities."
  },
  {
    "objectID": "analysis/4.5-folium.html",
    "href": "analysis/4.5-folium.html",
    "title": "Census API and Data Collection",
    "section": "",
    "text": "Code\n# Import packages\n\nimport altair as alt\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport pandas as pd\n#import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\nimport requests\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nfrom folium import plugins\nfrom shapely.geometry import Point\nimport xyzservices\nimport osmnx as ox\nimport networkx as nx\nimport pygris\nimport cenpy\n\n\n\n%matplotlib inline\n\n# See lots of columns\npd.options.display.max_rows = 9999 \npd.options.display.max_colwidth = 200\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensus API Data\nIn this section, an API query is generated to retrieve demographic data for San Francisco through the American Community Survey (ACS) 5-year survey for the year 2021. The variables selected for analysis include the white population, Hispanic or Latino population, median income, and the population that commutes by driving. These variables are deemed significant for understanding socioeconomic and commuting patterns after examining multiple variables. This information is brought in at the tract level to capture localized nuances. It is then joined to the working dataset to examine parking trends in the context of demographic associations at a granular level.\n\n\nCode\n#available = cenpy.explorer.available()\n#available.head()\n\n# Return a dataframe of all datasets that start with \"ACS\"\n# Axis=0 means to filter the index labels!\n#acs = available.filter(regex=\"^ACS\", axis=0)\n\n# Return a dataframe of all datasets that start with \"ACSDT5Y\"\n#available.filter(regex=\"^ACSDT5Y\", axis=0)\n#acs = cenpy.remote.APIConnection(\"ACSDT5Y2019\")\n#acs.variables.head(n=100)\n\n\n#looking for variables\n\n#income_matches = acs.varslike(\n#    pattern=\"MEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS\",\n#    by=\"concept\",  # searches along concept column\n#).sort_index()\n\n#race_matches\n\n#race_matches = acs.varslike(\n#        pattern=\"WHITE\",\n#    by=\"concept\",  # searches along concept column\n#).sort_index()\n\n#race_matches\n\n#drive choice\n\n#drive_matches = acs.varslike(\n#        pattern=\"transportation\",\n #   by=\"concept\",  # searches along concept column\n#).sort_index()\n\n#drive_matches\n\n\n\n\nCode\n#variables = [\n#    \"NAME\",\n #   \"B19013_001E\", # med inc\n #   \"B03002_001E\", # Total\n #   \"B03002_003E\", # Not Hispanic, White\n #   \"B03002_004E\", # Not Hispanic, Black\n #   \"B03002_005E\", # Not Hispanic, American Indian\n #   \"B03002_006E\", # Not Hispanic, Asian\n #   \"B03002_007E\", # Not Hispanic, Native Hawaiian\n #   \"B03002_008E\", # Not Hispanic, Other\n #   \"B03002_009E\", #  Two or More Races\n #   \"B03002_012E\"]  # hisp\n\n#Med_Age = B01002_001E,\n#     White_Pop = B02001_002E,\n#     Travel_Time = B08013_001E,\n#     Num_Commuters = B08012_001E,\n#     Means_of_Transport = B08301_001E,\n#     Total_Public_Trans = B08301_010E,\n#     workforce_16 = B08007_001E,\n#     Num_Vehicles = B06012_002E,\n\n\n#counties = cenpy.explorer.fips_table(\"COUNTY\")\n#counties.head()\n\n# Search for rows where name contains \"San Francisco\"\n#counties.loc[ counties[3].str.contains(\"San Francisco\") ]\n\n#sf_county_code = \"075\"\n#ca_state_code = \"06\"\n\n#sf_inc_data = acs.query(\n#    cols=variables,\n#    geo_unit=\"block group:*\",\n#    geo_filter={\"state\": ca_state_code, \"county\": sf_county_code, \"tract\": \"*\"},\n#)\n\n\n#sf_inc_data.head(700)\n\n\n\n\n\n\n\n\nNote\n\n\n\nAt this point in our analysis, we were able to collect race and income variables from the census API, but were running into errors while trying to include other variables invloving drive time to work and preferred mode of transportation. To fix this error, we performed the census API call in R and joined that data to our existing dataset. The variables that we were unable to join are commented on the code chunk above. The R script used is available on the project repository.\n\n\n\n\nCode\n#convert to float \n\n#for variable in variables:\n#    # Convert all variables EXCEPT for NAME\n#    if variable != \"NAME\":\n#        sf_inc_data[variable] = sf_inc_data[variable].astype(float)\n        \n        \n\n\n\n\nCode\n#merges\n#sf_inc_data.rename(columns={'B19013_001E': 'Median Income',  \"B03002_001E\": \"Total\",  # Total\n#        \"B03002_003E\": \"White\",  # Not Hispanic, White\n#        \"B03002_004E\": \"Black\",  # Not Hispanic, Black\n#        \"B03002_005E\": \"AI/AN\",  # Not Hispanic, American Indian\n#        \"B03002_006E\": \"Asian\",  # Not Hispanic, Asian\n#        \"B03002_007E\": \"NH/PI\",  # Not Hispanic, Native Hawaiian\n#        \"B03002_008E\": \"Other_\",  # Not Hispanic, Other\n#        \"B03002_009E\": \"Two Plus\",  # Not Hispanic, Two or More Races\n#        \"B03002_012E\": \"Hispanic\"}, inplace=True)\n\n#sf_inc_data = sf_inc_data.loc[sf_inc_data['Median Income'] &gt; 0]\n\n# sf_block_groups = pygris.block_groups(\n#     state=ca_state_code, county=sf_county_code, year=2021\n# )\n# sf_final = sf_block_groups.merge(\n#     sf_inc_data,\n#     left_on=[\"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\"],\n#     right_on=[\"state\", \"county\", \"tract\", \"block group\"],\n# )\n\n# #writing the geojson to use in r\n\n# #bringing back the complete dataset\n\n# #sf_final.to_file(output_file, driver='GeoJSON')\n\nsf_final = gpd.read_file(\"./data/census.geojson\")\n\nsf_final = gpd.sjoin(sf_final, sf_block_groups, how=\"inner\", op=\"intersects\")\n\ncolumns_to_drop = ['STATEFP', 'COUNTYFP', 'TRACTCE', 'BLKGRPCE', 'GEOID', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'ALAND', 'AWATER', 'INTPTLAT', 'INTPTLON','index_right']\n\nsf_final.drop(columns=columns_to_drop, inplace=True)\n\n#sf_final.head()\n\n#print(type(sf_final))\n\n\nD:\\Fall_2023\\Python\\Mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n\nCode\n#columns_to_drop = ['index_right']\n\n#sf_final.drop(columns=columns_to_drop, inplace=True)\n\n#sf_final.head()\n\n#column_names = sf_final.columns.tolist()\n#print(column_names)\n\n\n\n\nExploratory analysis of census variables\nFirst, the median income map is examined to discern patterns, if any, between neighborhood wealth and parking meter density. It is hard to draw any meaningful conclusions from this map alone, as we need to join the street and parking meter data to see where the overlaps occur.\n\nData by Census Tract\n\n\nCode\n# plot\n\n#sf_final.explore(column=\"Med_Inc\", tiles=\"cartodbdark_matter\")\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/6-folium.html",
    "href": "analysis/6-folium.html",
    "title": "Predictive Modelling",
    "section": "",
    "text": "We’ll extend our analysis to do a hyperparameter grid search to find the best model configuration. Choosing varibles based on correlations and piping those into random forest model. We can see here that factors like percentage of people taking public transit and residents having longer commute times have a negative correlation with number of parking meters per street, indicating that an increase in these variables can be associated with meter-rich areas.\n\n\nCode\n# Import packages\n\nimport altair as alt\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\nimport requests\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nfrom folium import plugins\nfrom shapely.geometry import Point\nimport xyzservices\nimport osmnx as ox\nimport networkx as nx\nimport pygris\nimport cenpy\n\nimport requests\nimport seaborn as sns\nimport contextily as ctx\n\n# Models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Model selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# Pipelines\nfrom sklearn.pipeline import make_pipeline\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnp.random.seed(42)\n\n\n\n%matplotlib inline\n\n# See lots of columns\npd.options.display.max_rows = 9999 \npd.options.display.max_colwidth = 200\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#bringing in data\n# joining census and OSM data \n\n#columns_to_drop = ['in']\n\n#sf_final.drop(columns=columns_to_drop, inplace=True)\nsf_final = gpd.read_file(\"./data/census.geojson\")\nmerged_gdf = gpd.read_file(\"./data/merged_gdf.geojson\")\n\nsf_final = sf_final.to_crs('EPSG:3857')\n\nmerged_gdf = merged_gdf.to_crs('EPSG:3857')\n\nfinal_gdf = gpd.sjoin(merged_gdf, sf_final, how='left', op='intersects')\n\n#columns_to_drop = ['STATEFP', 'COUNTYFP', 'TRACTCE', 'BLKGRPCE', 'GEOID', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'INTPTLAT', 'INTPTLON']\n\n#final_gdf.drop(columns=columns_to_drop, inplace=True)\n\n\n#final_gdf.head()\n\n\nD:\\Fall_2023\\Python\\Mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n\nCode\n#selecting the variables with good correlations\n\ncols = [\n    \"truecount\",\n 'Total_Pop', \n    'Travel_Time', \n    'Means_of_Transport', \n    'Total_Public_Trans', \n    'workforce_16', \n    'households_car_pct', \n    'commute_30_90min_pct', \n    'Drive_Work_pct'\n]\n\n# Trim to these columns and remove NaNs\nmodel_data = final_gdf[cols + [\"geometry\"]].dropna()\nmodel_data.head()\n\n\n\n\n\n\n\n\n\n\ntruecount\nTotal_Pop\nTravel_Time\nMeans_of_Transport\nTotal_Public_Trans\nworkforce_16\nhouseholds_car_pct\ncommute_30_90min_pct\nDrive_Work_pct\ngeometry\n\n\n\n\n0\n0.350877\n7621.0\n135615.0\n4821.0\n1299.0\n4821.0\n0.240937\n0.416719\n0.196225\nLINESTRING (-13623984.058 4546847.107, -13623982.756 4546832.729)\n\n\n1\n0.090023\n4218.0\n59975.0\n2653.0\n554.0\n2653.0\n0.120824\n0.390124\n0.354693\nLINESTRING (-13629159.824 4541826.151, -13629200.645 4541836.792)\n\n\n2\n0.087558\n3772.0\n54970.0\n2319.0\n467.0\n2319.0\n0.414820\n0.339802\n0.149202\nLINESTRING (-13625599.493 4548143.825, -13625630.050 4548174.641)\n\n\n3\n0.488599\n2024.0\n41265.0\n1402.0\n464.0\n1402.0\n0.211877\n0.399429\n0.048502\nLINESTRING (-13627260.424 4549700.424, -13627229.711 4549705.397)\n\n\n4\n0.031525\n3772.0\n54970.0\n2319.0\n467.0\n2319.0\n0.414820\n0.339802\n0.149202\nLINESTRING (-13625103.854 4548726.940, -13625181.255 4548634.529)"
  },
  {
    "objectID": "analysis/6-folium.html#splitting-the-data",
    "href": "analysis/6-folium.html#splitting-the-data",
    "title": "Predictive Modelling",
    "section": "Splitting the data",
    "text": "Splitting the data\nSplit random 30/70\n\n\nCode\n# Split the data 70/30\ntrain_set, test_set = train_test_split(model_data, test_size=0.3, random_state=42)\n\n# the target labels\ny_train = train_set[\"truecount\"]\ny_test = test_set[\"truecount\"]\n\n#y_test.head()\n\n\n2511    0.122466\n3603    0.326433\n1275    0.019455\n1237    0.059726\n2502    0.129769\nName: truecount, dtype: float64\n\n\ntransforming columns\n\n\nCode\n# Set up the column transformer with two transformers\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), cols)\n    ]\n)\n\n\n\n\nCode\npipe = make_pipeline(\n    transformer, RandomForestRegressor(random_state=42)\n)\npipe.fit(train_set, y_train);\npipe.score(test_set, y_test)\n\n\n0.9903700341139564\n\n\nOptimize hyperparameters\n\n\nCode\nparam_grid = {\n    \"randomforestregressor__n_estimators\": [5, 10, 25, 50, 100, 200], #Reduce estimators to reduce run time\n    \"randomforestregressor__max_depth\": [None, 2, 5, 7, 9],#Reduce depth to reduce run time\n}\n\ngrid = GridSearchCV(pipe, param_grid, cv=10, n_jobs=-1)  # Utilize all available processors\n\n# Run the search\ngrid.fit(train_set, y_train)\n\n\nGridSearchCV(cv=10,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['truecount',\n                                                                          'Total_Pop',\n                                                                          'Travel_Time',\n                                                                          'Means_of_Transport',\n                                                                          'Total_Public_Trans',\n                                                                          'workforce_16',\n                                                                          'households_car_pct',\n                                                                          'commute_30_90min_pct',\n                                                                          'Drive_Work_pct'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [None, 2, 5, 7, 9],\n                         'randomforestregressor__n_estimators': [5, 10, 25, 50,\n                                                                 100, 200]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=10,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['truecount',\n                                                                          'Total_Pop',\n                                                                          'Travel_Time',\n                                                                          'Means_of_Transport',\n                                                                          'Total_Public_Trans',\n                                                                          'workforce_16',\n                                                                          'households_car_pct',\n                                                                          'commute_30_90min_pct',\n                                                                          'Drive_Work_pct'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [None, 2, 5, 7, 9],\n                         'randomforestregressor__n_estimators': [5, 10, 25, 50,\n                                                                 100, 200]})estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['truecount', 'Total_Pop',\n                                                   'Travel_Time',\n                                                   'Means_of_Transport',\n                                                   'Total_Public_Trans',\n                                                   'workforce_16',\n                                                   'households_car_pct',\n                                                   'commute_30_90min_pct',\n                                                   'Drive_Work_pct'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['truecount', 'Total_Pop', 'Travel_Time',\n                                  'Means_of_Transport', 'Total_Public_Trans',\n                                  'workforce_16', 'households_car_pct',\n                                  'commute_30_90min_pct', 'Drive_Work_pct'])])num['truecount', 'Total_Pop', 'Travel_Time', 'Means_of_Transport', 'Total_Public_Trans', 'workforce_16', 'households_car_pct', 'commute_30_90min_pct', 'Drive_Work_pct']StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\nCode\nbest_random = grid.best_estimator_\ngrid.score(test_set, y_test)\n\n\n0.999872947311154\n\n\n\n\nCode\nprint(\"Best R^2 score:\", grid.best_score_)\n\n\nBest R^2 score: 0.9999591625411464\n\n\nHigh R2: model is likely overfit\n\n\nCode\n# Predictions for log of sales price\nlog_predictions = grid.best_estimator_.predict(test_set)\n\nX = model_data.loc[test_set.index]\n\n# Convert the predicted test values from log\nX['prediction'] = np.exp(log_predictions)\n\n#X.head()\n\n\nValueError: Length of values (1615) does not match length of index (1751)\n\n\n\n# Ensure test_set indices are in model_data\ncommon_indices = test_set.index.intersection(model_data.index)\n\n# Filter model_data based on common indices\nX = model_data.loc[common_indices]\n\n# Check the length of log_predictions and assign to 'prediction' column\nif len(log_predictions) == len(common_indices):\n    X['prediction'] = np.exp(log_predictions)\nelse:\n    print(\"Length mismatch between log_predictions and common_indices.\")\n\n#X.head()\n\nLength mismatch between log_predictions and common_indices."
  },
  {
    "objectID": "analysis/4-folium.html#introduction",
    "href": "analysis/4-folium.html#introduction",
    "title": "Analyzing Parking Spaces in LA",
    "section": "Introduction",
    "text": "Introduction\nIntroduction Being the second-largest city in the United States, Los Angeles is known for its heavy vehicle reliance, huge population density, and commuting culture. Downtown LA, where most commuters head to every single morning for work, has been a classic case study for urban planning for its complex outdated urban layout as well as high concentration of crimes. As thousands of commuters, residents, and visitors travel every day in this sophisticated neighborhood, street parking plays a crucial role in keeping the area’s ground transportation running. Therefore, being able to identify areas that are safe and accessible is essential for anyone who, for any reason, has to park in downtown LA. This study aims to assist drivers with a need for parking in streets of downtown LA by suggesting the drivers with the most optimal parking areas using the “Recommended Parking Score”. This score takes into consideration not only the safety of the streets, but also the price of available street parking meters. To achieve this, we need to take a comprehensive look at the neighborhood’s demographics, the trend of car-related crimes, as well as other the price distribution of parking meters in the area. In the very end, we hope to draw insights that might also help planners and policymakers fixing some of these problems we identified in our study.\n\nFile setup and data collection\nThe first step of this analysis comprises the essential tasks of loading necessary packages, configuring different APIs for data collection, and managing global environment settings.\n\n\nCode\n# Import packages\n\nimport altair as alt\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport pandas as pd\n#import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\nimport requests\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nfrom folium import plugins\nfrom shapely.geometry import Point\nimport xyzservices\nimport osmnx as ox\nimport networkx as nx\nimport pygris\nimport cenpy\n\n\n\n%matplotlib inline\n\n# See lots of columns\npd.options.display.max_rows = 9999 \npd.options.display.max_colwidth = 200\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\n\n\nData Wrangling\nThis step involves gathering data on parking for 2023 and preliminary data cleaning for the large dataset. All geospatial datasets are set to a uniform coordinate reference system, and boundary shapefiles are primed for use in OSM street network API.\n\nmeters = pd.read_csv('/Users/bin/Downloads/LADOT_Metered_Parking_Inventory___Policies_20241222.csv')\nprint(meters.columns)\nmeters.head()\n\nIndex(['SpaceID', 'BlockFace', 'MeterType', 'RateType', 'RateRange',\n       'MeteredTimeLimit', 'LatLng'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nSpaceID\nBlockFace\nMeterType\nRateType\nRateRange\nMeteredTimeLimit\nLatLng\n\n\n\n\n0\nWW516\n650 HARVARD BLVD\nSingle-Space\nFLAT\n$1.00\n2HR\n(34.060385, -118.304103)\n\n\n1\nCB3034\n201 E 4TH ST\nSingle-Space\nTOD\n$1.00 - $6.00\n2HR\n(34.047109, -118.245841)\n\n\n2\nBH398\n1901 1ST ST\nSingle-Space\nFLAT\n$1.00\n1HR\n(34.045795, -118.21555)\n\n\n3\nUC8\n3701 N CAHUENGA BLVD\nSingle-Space\nFLAT\n$1.00\n1HR\n(34.136733, -118.363025)\n\n\n4\nCB2345\n1401 S SAN PEDRO ST\nSingle-Space\nTOD\n$0.50 - $1.00\n4HR\n(34.030958, -118.255362)\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport numpy as np\n\n# Suppress warnings for invalid operations\nnp.seterr(invalid=\"ignore\")\n\n# Load parking meter data\nmeters = pd.read_csv('/Users/bin/Downloads/LADOT_Metered_Parking_Inventory___Policies_20241222.csv')\n\n# Remove parentheses and split LatLng into separate Latitude and Longitude columns\nmeters['LatLng'] = meters['LatLng'].str.strip(\"()\")\nmeters[['LATITUDE', 'LONGITUDE']] = meters['LatLng'].str.split(', ', expand=True).astype(float)\n\n# Convert to GeoDataFrame\ngeometry = [Point(xy) for xy in zip(meters['LONGITUDE'], meters['LATITUDE'])]\nmeters = gpd.GeoDataFrame(meters, geometry=geometry)\nmeters.crs = 'EPSG:4326'\n\n# Reproject to Web Mercator\nmeters = meters.to_crs('EPSG:3857')\n\n# Check the first few rows of the GeoDataFrame\n# print(meters.head())\n\n\nParking meters in LA\nHere is an overview of all parking meters in the city of Los Angeles and its adjacent cities. We observe the concentration of parking meters in central Los Angeles is significantly higher than its surrounding neighborhood. As shown by the map, most of the parking meters are located in downtown LA, followed by Fair Fax then Korean town where the demand of street parking is high. However, this map alone dose not provide enough information for us to draw useful conclusions in an urbanized area with such complexity as the distribution of parking meters might be determined by population, profitability, parking demand, planning, and policy making. Therefore, we need to look at the problem using additional lenses.\n\nimport folium\nfrom folium.plugins import FastMarkerCluster\nimport xyzservices.providers\n\n# Extract LATITUDE and LONGITUDE columns\ncoords = meters[[\"LATITUDE\", \"LONGITUDE\"]].values.tolist()  # Convert to list of (lat, lon)\n\n# Create a map centered on Los Angeles with light mode\nm = folium.Map(\n    location=[34.05, -118.25],  # Center on Los Angeles\n    zoom_start=12,\n    tiles=xyzservices.providers.CartoDB.Positron  # Light mode tiles\n)\n\n# Add a FastMarkerCluster with the meter locations\nFastMarkerCluster(data=coords).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nOpen Street Map Data\nTo streamline the workflow with this large dataset, relevant OSM data is refined by excluding highways, where parking is not allowed. This ensures the dataset focuses solely on accessible areas with available parking spaces. A new graph is created and plotted to reflect only the non-highway streets.\n\nimport osmnx as ox\n\n# Define a smaller area or bounding box\ncity_name = 'Los Angeles, California, USA'\n\n# Retrieve the graph with simplification and largest component only\nG = ox.graph.graph_from_place(city_name, network_type='drive', simplify=True, retain_all=False)\n\n# Optional: Plot the graph\nox.plot_graph(G, bgcolor='k', node_color='w', node_size=5, edge_color='w', edge_linewidth=0.5)",
    "crumbs": [
      "Analysis",
      "Analyzing Parking Spaces in LA"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#number-of-parking-meters-by-length-per-street",
    "href": "analysis/4-folium.html#number-of-parking-meters-by-length-per-street",
    "title": "Analyzing Parking Spaces in LA",
    "section": "Number of Parking Meters by Length per Street",
    "text": "Number of Parking Meters by Length per Street\nThe following map shows the number of parking meters by street segments in the city of Los Angeles with a focus on downtown LA area. Interestingly, even though we observe most of the parking meters by street in downtown, the segments with higher density seem to be dispersed across the area. In addition to downtown, other areas such as Hollywood, Beverly Hills, Culver City, and around Santa Monica. The closer they are from downtown LA, the more disperse they seem to be. There are also noticeable segments expanding northwest of the city of Los Angeles towards Calabasas and Burbank. Very few segments are present to the east and south of downtown LA.\n\n# Filter out highways (e.g., motorways, trunk roads)\nnon_highway_edges = [(u, v, key) for u, v, key, data in G.edges(keys=True, data=True) if 'highway' not in data or 'highway' in data and data['highway'] != 'motorway']\n\n# Create a new graph with non-highway streets\nG = G.edge_subgraph(non_highway_edges)\n\n# Plot the non-highway street network\n#ox.plot_graph(G, bgcolor='k', node_color='w', edge_color='w', node_size=5, edge_linewidth=0.5)\n\n\n# Step 1: Convert the graph to a GeoDataFrame containing edges\nla_edges = ox.graph_to_gdfs(G, edges=True, nodes=False)\n\n# Step 2: Project the graph to the EPSG:3857 coordinate reference system\nG_projected = ox.project_graph(G, to_crs='EPSG:3857')\n\n# Step 3: Extract longitude and latitude coordinates from the 'meters' GeoDataFrame\nx_coords = meters['geometry'].x  # Longitude in meters\ny_coords = meters['geometry'].y  # Latitude in meters\n\n# Step 4: Find the nearest edges for each parking meter\nnearest_edges = ox.distance.nearest_edges(G_projected, X=x_coords, Y=y_coords)\n\n# Step 5: Create a DataFrame with edge identifiers and count occurrences\nmeters_nodes = pd.DataFrame(nearest_edges, columns=['u', 'v', 'key'])\nmeters_nodes['Count'] = 1\n\n# Step 6: Group by edge identifiers and calculate total counts\ngrouped_counts = meters_nodes.groupby(['u', 'v'])['Count'].sum().reset_index()\n\n# Step 7: Merge edge counts with the edges GeoDataFrame\nmerged_gdf = la_edges.merge(grouped_counts, on=['u', 'v'], how='left')\n\n# Step 8: Filter rows with non-zero counts\nmerged_gdf = merged_gdf[merged_gdf['Count'] &gt; 0]\n\n# Step 9: Drop unnecessary columns for cleaner data\ncolumns_to_remove = [\n    'u', 'v', 'osmid', 'oneway', 'lanes', 'ref', 'maxspeed', \n    'reversed', 'access', 'bridge', 'junction', 'width', 'tunnel'\n]\nmerged_gdf = merged_gdf.drop(columns=columns_to_remove)\n\n# Step 10: Calculate the normalized count ('truecount')\nmerged_gdf['truecount'] = merged_gdf['Count'] / merged_gdf['length']\n\n# Step 11: Filter out edges with lengths outside the range [10, 100]\nlength_filter = (merged_gdf['length'] &gt;= 10) & (merged_gdf['length'] &lt;= 100)\nmerged_gdf = merged_gdf[length_filter]\n\n\nmerged_gdf.explore(tiles='cartodbdark_matter', column = 'truecount')\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Analyzing Parking Spaces in LA"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#parking-price-in-la",
    "href": "analysis/4-folium.html#parking-price-in-la",
    "title": "Analyzing Parking Spaces in LA",
    "section": "Parking Price in LA",
    "text": "Parking Price in LA\nTaking a closer look at the street parking prices in the city of Los Angeles, we observe the most expensive parking in downtown LA which can be as expensive as 6 times the price of adjacent areas. This makes total sense since downtown LA gathers the most high-income working-class commuters who demand parking the most, making the area most profitable.\n\nimport pandas as pd\nimport folium\nfrom folium.plugins import MarkerCluster\nimport xyzservices.providers\n\n# Example data with RateRange added\ndata = {\n    \"SpaceID\": [\"WW516\", \"CB3034\", \"BH398\"],\n    \"MeteredTimeLimit\": [\"2HR\", \"2HR\", \"1HR\"],\n    \"RateRange\": [\"$1.00\", \"$1.00 - $6.00\", \"$1.00\"],\n    \"LATITUDE\": [34.060385, 34.047109, 34.045795],\n    \"LONGITUDE\": [-118.304103, -118.245841, -118.21555],\n}\n\n# Convert to DataFrame\nmeters = pd.DataFrame(data)\n\n# Convert MeteredTimeLimit to numeric hours (e.g., \"2HR\" -&gt; 2)\nmeters['TimeLimit'] = meters['MeteredTimeLimit'].str.extract('(\\d+)').astype(float)\n\n# Convert RateRange to numeric (use midpoint for ranges)\ndef extract_rate(rate_range):\n    rates = [float(r.replace(\"$\", \"\")) for r in rate_range.split(\" - \")]\n    return sum(rates) / len(rates)  # Use midpoint\n\nmeters['RateValue'] = meters['RateRange'].apply(extract_rate)\n\n# Create a map centered on Los Angeles with light mode\nm = folium.Map(\n    location=[34.05, -118.25],  # Center on Los Angeles\n    zoom_start=12,\n    tiles=xyzservices.providers.CartoDB.Positron  # Light mode tiles\n)\n\n# Add CircleMarkers to the map with size based on RateValue\nfor _, row in meters.iterrows():\n    folium.CircleMarker(\n        location=[row['LATITUDE'], row['LONGITUDE']],\n        radius=row['RateValue'] * 5,  # Scale the size\n        color=\"blue\",\n        fill=True,\n        fill_opacity=0.6,\n        popup=f\"Metered Time Limit: {row['MeteredTimeLimit']}&lt;br&gt;Rate Range: {row['RateRange']}&lt;br&gt;Rate Value: ${row['RateValue']:.2f}\"\n    ).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# parking meters heated map\nimport folium\nfrom folium.plugins import HeatMap\nimport pandas as pd\n\n# Sample data as a DataFrame (replace with your actual data)\ndata = {\n    \"SpaceID\": [\"WW516\", \"CB3034\", \"BH398\", \"UC8\", \"CB2345\"],\n    \"LATITUDE\": [34.060385, 34.047109, 34.045795, 34.136733, 34.030958],\n    \"LONGITUDE\": [-118.304103, -118.245841, -118.215550, -118.363025, -118.255362],\n}\nmeters = pd.DataFrame(data)\n\n# Extract coordinates for the heatmap\ncoordinates = meters[[\"LATITUDE\", \"LONGITUDE\"]].values.tolist()\n\n# Create a map centered on Los Angeles with a light mode basemap\nm = folium.Map(location=[34.05, -118.25], zoom_start=12, tiles=\"CartoDB Positron\")\n\n# Add heatmap layer\nHeatMap(coordinates).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nimport pandas as pd\n\n# API URL for the CSV data\nurl = \"https://data.lacity.org/resource/2nrs-mtv8.csv\"\n\n# Read the CSV file from the API\ncrime_data = pd.read_csv(url)\n\ncrime_data.head()\n\n\n\n\n\n\n\n\ndr_no\ndate_rptd\ndate_occ\ntime_occ\narea\narea_name\nrpt_dist_no\npart_1_2\ncrm_cd\ncrm_cd_desc\n...\nstatus\nstatus_desc\ncrm_cd_1\ncrm_cd_2\ncrm_cd_3\ncrm_cd_4\nlocation\ncross_street\nlat\nlon\n\n\n\n\n0\n190326475\n2020-03-01T00:00:00.000\n2020-03-01T00:00:00.000\n2130\n7\nWilshire\n784\n1\n510\nVEHICLE - STOLEN\n...\nAA\nAdult Arrest\n510\n998.0\nNaN\nNaN\n1900 S LONGWOOD AV\nNaN\n34.0375\n-118.3506\n\n\n1\n200106753\n2020-02-09T00:00:00.000\n2020-02-08T00:00:00.000\n1800\n1\nCentral\n182\n1\n330\nBURGLARY FROM VEHICLE\n...\nIC\nInvest Cont\n330\n998.0\nNaN\nNaN\n1000 S FLOWER ST\nNaN\n34.0444\n-118.2628\n\n\n2\n200320258\n2020-11-11T00:00:00.000\n2020-11-04T00:00:00.000\n1700\n3\nSouthwest\n356\n1\n480\nBIKE - STOLEN\n...\nIC\nInvest Cont\n480\nNaN\nNaN\nNaN\n1400 W 37TH ST\nNaN\n34.0210\n-118.3002\n\n\n3\n200907217\n2023-05-10T00:00:00.000\n2020-03-10T00:00:00.000\n2037\n9\nVan Nuys\n964\n1\n343\nSHOPLIFTING-GRAND THEFT ($950.01 & OVER)\n...\nIC\nInvest Cont\n343\nNaN\nNaN\nNaN\n14000 RIVERSIDE DR\nNaN\n34.1576\n-118.4387\n\n\n4\n200412582\n2020-09-09T00:00:00.000\n2020-09-09T00:00:00.000\n630\n4\nHollenbeck\n413\n1\n510\nVEHICLE - STOLEN\n...\nIC\nInvest Cont\n510\nNaN\nNaN\nNaN\n200 E AVENUE 28\nNaN\n34.0820\n-118.2130\n\n\n\n\n5 rows × 28 columns",
    "crumbs": [
      "Analysis",
      "Analyzing Parking Spaces in LA"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#vehicle-stolen-crimes-point",
    "href": "analysis/4-folium.html#vehicle-stolen-crimes-point",
    "title": "Analyzing Parking Spaces in LA",
    "section": "Vehicle Stolen Crimes Point",
    "text": "Vehicle Stolen Crimes Point\nIn this section, we will be analyzing car-related crimes in the city of Los Angeles, specifically examining vehicle stealing incidents. According to the 2020 crime data, most cars are stolen from central Los Angeles in addition to relatively smaller clusters around San Fernando and Long Beach.\n\n# vehicle related crime map\nimport pandas as pd\nimport folium\n\n# API URL for the CSV data\nurl = \"https://data.lacity.org/resource/2nrs-mtv8.csv\"\n\n# Read the CSV file from the API\ncrime_data = pd.read_csv(url)\n\n# Filter for \"VEHICLE - STOLEN\" cases\nvehicle_stolen_data = crime_data[crime_data['crm_cd_desc'] == \"VEHICLE - STOLEN\"]\n\n# Check for valid lat/lon data and drop rows with missing coordinates\nvehicle_stolen_data = vehicle_stolen_data.dropna(subset=['lat', 'lon'])\n\n# Convert lat/lon to numeric (in case they are read as strings)\nvehicle_stolen_data['lat'] = pd.to_numeric(vehicle_stolen_data['lat'])\nvehicle_stolen_data['lon'] = pd.to_numeric(vehicle_stolen_data['lon'])\n\n# Create a map centered on Los Angeles with a light mode basemap\nm = folium.Map(location=[34.05, -118.25], zoom_start=12, tiles=\"CartoDB Positron\")\n\n# Add markers for each \"VEHICLE - STOLEN\" case\nfor _, row in vehicle_stolen_data.iterrows():\n    folium.Marker(\n        location=[row['lat'], row['lon']],\n        popup=f\"Location: {row['location']}&lt;br&gt;Date: {row['date_occ']}\"\n    ).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Analyzing Parking Spaces in LA"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#vehicle-stolen-heat-map",
    "href": "analysis/4-folium.html#vehicle-stolen-heat-map",
    "title": "Analyzing Parking Spaces in LA",
    "section": "Vehicle Stolen Heat Map",
    "text": "Vehicle Stolen Heat Map\nThis trend can be better represented by a heatmap that captures the location where a car was stolen. For the purpose of this study, we will be looking at the city of Los Angeles in particular. Shown by the map, the highest number of vehicles stolen incidents are located right around downtown LA and expands towards its surrounding neighborhoods such as Culver City, Inglewood, and parts of Pasadena. With downtown LA being the origin of the heat, it is reasonable for us to draw a conclusion that downtown LA demands the most attention and is considered the most dangerous when it comes to vehicle-related crimes.\n\nimport pandas as pd\nimport folium\nfrom folium.plugins import HeatMap\n\n# API URL for the CSV data\nurl = \"https://data.lacity.org/resource/2nrs-mtv8.csv\"\n\n# Read the CSV file from the API\ncrime_data = pd.read_csv(url)\n\n# Filter for \"VEHICLE - STOLEN\" cases\nvehicle_stolen_data = crime_data[crime_data['crm_cd_desc'] == \"VEHICLE - STOLEN\"]\n\n# Drop rows with missing coordinates\nvehicle_stolen_data = vehicle_stolen_data.dropna(subset=['lat', 'lon'])\n\n# Convert lat/lon to numeric\nvehicle_stolen_data['lat'] = pd.to_numeric(vehicle_stolen_data['lat'])\nvehicle_stolen_data['lon'] = pd.to_numeric(vehicle_stolen_data['lon'])\n\n# Extract latitude and longitude as a list of [lat, lon]\nheat_data = vehicle_stolen_data[['lat', 'lon']].values.tolist()\n\n# Create a map centered on Los Angeles with a light mode basemap\nm = folium.Map(location=[34.05, -118.25], zoom_start=12, tiles=\"CartoDB Positron\")\n\n# Add the heatmap layer\nHeatMap(heat_data, radius=10).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# Import packages\n\nimport altair as alt\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport pandas as pd\n#import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\nimport requests\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nfrom folium import plugins\nfrom shapely.geometry import Point\nimport xyzservices\nimport osmnx as ox\nimport networkx as nx\nimport pygris\nimport cenpy\n\n\n\n%matplotlib inline\n\n# See lots of columns\npd.options.display.max_rows = 9999 \npd.options.display.max_colwidth = 200\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\navailable = cenpy.explorer.available()\navailable.head()\n\n# Return a dataframe of\n\n/Users/bin/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/cenpy/explorer.py:70: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  listcols = raw_table.applymap(lambda x: isinstance(x, list)).any()\n\n\n\n\n\n\n\n\n\nc_isTimeseries\nc_isMicrodata\npublisher\ntemporal\nspatial\nprogramCode\nmodified\nkeyword\ncontactPoint\ndistribution\ndescription\nbureauCode\naccessLevel\ntitle\nc_isAvailable\nc_isCube\nc_isAggregate\nc_dataset\nvintage\n\n\n\n\nABSCB2017\nNaN\nNaN\nU.S. Census Bureau\n2017/2017\nUnited States\n006:007\n2020-04-30 00:00:00.0\n(census,)\n{'fn': 'ABS Staff', 'hasEmail': 'mailto:adep.annual.business.survey@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2017/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, ...\n006:07\npublic\nAnnual Business Survey: Characteristics of Businesses: 2017\nTrue\nNaN\nTrue\n(abscb,)\n2017.0\n\n\nABSCB2018\nNaN\nNaN\nU.S. Census Bureau\n2018/2018\nUnited States\n006:007\n2020-10-26 00:00:00.0\n(census,)\n{'fn': 'ABS Staff', 'hasEmail': 'mailto:adep.annual.business.survey@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2018/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, ...\n006:07\npublic\nAnnual Business Survey: Characteristics of Businesses: 2018\nTrue\nNaN\nTrue\n(abscb,)\n2018.0\n\n\nABSCB2019\nNaN\nNaN\nU.S. Census Bureau\n2019/2019\nUS\n006:007\n2021-08-17 00:00:00.0\n(census,)\n{'fn': 'ASE Staff', 'hasEmail': 'mailto:ERD.annual.survey.of.entrepreneurs@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2019/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, ...\n006:07\npublic\n2019 Annual Business Survey: Characteristics of Business\nTrue\nNaN\nTrue\n(abscb,)\n2019.0\n\n\nABSCB2020\nNaN\nNaN\nU.S. Census Bureau\n2020/2020\nUS\n006:007\n2022-08-03 00:00:00.0\n(census,)\n{'fn': 'ASE Staff', 'hasEmail': 'mailto:ERD.annual.survey.of.entrepreneurs@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2020/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, ...\n006:07\npublic\n2020 Annual Business Survey: Characteristics of Business\nTrue\nNaN\nTrue\n(abscb,)\n2020.0\n\n\nABSCB2021\nNaN\nNaN\nU.S. Census Bureau\n2021/2021\nUnited States\n006:007\n2023-07-24 10:30:52.0\n(census,)\n{'fn': 'ABS Staff', 'hasEmail': 'mailto:adep.annual.business.survey@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2021/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, ...\n006:07\npublic\n2021 Annual Business Survey: Characteristics of Business\nTrue\nNaN\nTrue\n(abscb,)\n2021.0\n\n\n\n\n\n\n\n\n# Connect to Census API\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\n\n# Define variables of interest\nvariables = [\n    \"NAME\",\n    \"B19013_001E\",  # Median income\n    \"B03002_001E\",  # Total population\n    \"B03002_003E\",  # Not Hispanic, White\n    \"B03002_012E\",  # Hispanic or Latino\n    \"B08301_001E\",  # Means of transportation to work\n    \"B08301_010E\"   # Public transportation\n]\n\n# Los Angeles County and California codes\nla_county_code = \"037\"\nca_state_code = \"06\"\n\n# Query ACS data for Los Angeles block groups\nla_inc_data = acs.query(\n    cols=variables,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": ca_state_code, \"county\": la_county_code, \"tract\": \"*\"}\n)\n\n# Convert numerical columns to float\nfor variable in variables:\n    if variable != \"NAME\":\n        la_inc_data[variable] = la_inc_data[variable].astype(float)\n\n\n# Create a cleaned DataFrame from la_inc_data\nla_final = la_inc_data.copy()\n\n# Adjusted columns to drop\ncolumns_to_drop = [\n    \"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\", \"GEOID\", \"NAMELSAD\",\n    \"MTFCC\", \"FUNCSTAT\", \"ALAND\", \"AWATER\", \"INTPTLAT\", \"INTPTLON\"\n]\n\n# Drop unnecessary columns\nif all(col in la_final.columns for col in columns_to_drop):\n    la_final.drop(columns=columns_to_drop, inplace=True)\nelse:\n    missing_cols = [col for col in columns_to_drop if col not in la_final.columns]\n    print(f\"Warning: The following columns are missing and cannot be dropped: {missing_cols}\")\n\n# Verify the structure of the cleaned DataFrame\nprint(la_final.columns)\nla_final.head()\n\nWarning: The following columns are missing and cannot be dropped: ['STATEFP', 'COUNTYFP', 'TRACTCE', 'BLKGRPCE', 'GEOID', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'ALAND', 'AWATER', 'INTPTLAT', 'INTPTLON']\nIndex(['NAME', 'B19013_001E', 'B03002_001E', 'B03002_003E', 'B03002_012E',\n       'B08301_001E', 'B08301_010E', 'state', 'county', 'tract',\n       'block group'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nNAME\nB19013_001E\nB03002_001E\nB03002_003E\nB03002_012E\nB08301_001E\nB08301_010E\nstate\ncounty\ntract\nblock group\n\n\n\n\n0\nBlock Group 1, Census Tract 1011.10, Los Angeles County, California\n63242.0\n1630.0\n932.0\n571.0\n697.0\n13.0\n06\n037\n101110\n1\n\n\n1\nBlock Group 2, Census Tract 1011.10, Los Angeles County, California\n56250.0\n1492.0\n864.0\n314.0\n772.0\n47.0\n06\n037\n101110\n2\n\n\n2\nBlock Group 3, Census Tract 1011.10, Los Angeles County, California\n99567.0\n757.0\n509.0\n120.0\n468.0\n0.0\n06\n037\n101110\n3\n\n\n3\nBlock Group 1, Census Tract 1011.22, Los Angeles County, California\n120833.0\n2608.0\n1879.0\n117.0\n1195.0\n0.0\n06\n037\n101122\n1\n\n\n4\nBlock Group 2, Census Tract 1011.22, Los Angeles County, California\n90536.0\n1639.0\n1061.0\n222.0\n782.0\n9.0\n06\n037\n101122\n2\n\n\n\n\n\n\n\n\n# Rename columns for easier interpretation\nla_final.rename(columns={\n    \"B19013_001E\": \"Median Income\",\n    \"B03002_001E\": \"Total Population\",\n    \"B03002_003E\": \"White Population\",\n    \"B03002_012E\": \"Hispanic Population\",\n    \"B08301_001E\": \"Total Commuters\",\n    \"B08301_010E\": \"Public Transit Commuters\"\n}, inplace=True)\n\n\nimport geopandas as gpd\nimport pygris\n\n# Step 1: Import block group geometries\nblock_groups = pygris.block_groups(state=\"CA\", county=\"037\", year=2021)\n\n# Step 2: Rename columns in block_groups to match la_final\nblock_groups.rename(\n    columns={\n        \"STATEFP\": \"state\",\n        \"COUNTYFP\": \"county\",\n        \"TRACTCE\": \"tract\",\n        \"BLKGRPCE\": \"block group\"\n    },\n    inplace=True\n)\n\n# Step 3: Merge with your dataset\nla_final_geo = block_groups.merge(la_final, on=[\"state\", \"county\", \"tract\", \"block group\"], how=\"left\")\n\n# Step 4: Set the CRS\nla_final_geo.crs = \"EPSG:4326\"\n\n# Step 5: Visualize Median Income\nla_final_geo.explore(\n    column=\"Median Income\",\n    tiles=\"cartodbdark_matter\",\n    legend=True\n)\n\nUsing FIPS code '06' for input 'CA'\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Check if the necessary columns exist\nif 'lat' in crime_data.columns and 'lon' in crime_data.columns:\n    # Create geometry from latitude and longitude\n    crime_data['geometry'] = gpd.points_from_xy(crime_data['lon'], crime_data['lat'])\n    \n    # Convert to GeoDataFrame\n    crime_gdf = gpd.GeoDataFrame(crime_data, geometry='geometry', crs=\"EPSG:4326\")\n    \n    # Check the first few rows to confirm\n    print(crime_gdf.head())\nelse:\n    print(\"Latitude ('lat') or Longitude ('lon') columns are missing in the dataset.\")\n\n       dr_no                date_rptd                 date_occ  time_occ  \\\n0  190326475  2020-03-01T00:00:00.000  2020-03-01T00:00:00.000      2130   \n1  200106753  2020-02-09T00:00:00.000  2020-02-08T00:00:00.000      1800   \n2  200320258  2020-11-11T00:00:00.000  2020-11-04T00:00:00.000      1700   \n3  200907217  2023-05-10T00:00:00.000  2020-03-10T00:00:00.000      2037   \n4  200412582  2020-09-09T00:00:00.000  2020-09-09T00:00:00.000       630   \n\n   area   area_name  rpt_dist_no  part_1_2  crm_cd  \\\n0     7    Wilshire          784         1     510   \n1     1     Central          182         1     330   \n2     3   Southwest          356         1     480   \n3     9    Van Nuys          964         1     343   \n4     4  Hollenbeck          413         1     510   \n\n                                crm_cd_desc  ...   status_desc  crm_cd_1  \\\n0                          VEHICLE - STOLEN  ...  Adult Arrest       510   \n1                     BURGLARY FROM VEHICLE  ...   Invest Cont       330   \n2                             BIKE - STOLEN  ...   Invest Cont       480   \n3  SHOPLIFTING-GRAND THEFT ($950.01 & OVER)  ...   Invest Cont       343   \n4                          VEHICLE - STOLEN  ...   Invest Cont       510   \n\n  crm_cd_2 crm_cd_3  crm_cd_4                                  location  \\\n0    998.0      NaN       NaN   1900 S  LONGWOOD                     AV   \n1    998.0      NaN       NaN   1000 S  FLOWER                       ST   \n2      NaN      NaN       NaN   1400 W  37TH                         ST   \n3      NaN      NaN       NaN  14000    RIVERSIDE                    DR   \n4      NaN      NaN       NaN                          200 E  AVENUE 28   \n\n   cross_street      lat       lon                     geometry  \n0           NaN  34.0375 -118.3506  POINT (-118.35060 34.03750)  \n1           NaN  34.0444 -118.2628  POINT (-118.26280 34.04440)  \n2           NaN  34.0210 -118.3002  POINT (-118.30020 34.02100)  \n3           NaN  34.1576 -118.4387  POINT (-118.43870 34.15760)  \n4           NaN  34.0820 -118.2130  POINT (-118.21300 34.08200)  \n\n[5 rows x 29 columns]\n\n\nimport geopandas as gpd from shapely.geometry import Point, Polygon",
    "crumbs": [
      "Analysis",
      "Analyzing Parking Spaces in LA"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#parking-recommendation-score-map",
    "href": "analysis/4-folium.html#parking-recommendation-score-map",
    "title": "Analyzing Parking Spaces in LA",
    "section": "Parking Recommendation Score Map",
    "text": "Parking Recommendation Score Map\nCombing the results from the previous sections, we attempted to create a map that takes consideration of both affordability and safety of parking in area with the highest risks which is downtown LA. This map provides drivers with the need for street parking in downtown LA with parking recommendation scores in the area. A lower score, marks by the red color, means that it is likely that the area is considered both dangerous and expensive for street parking while a higher score, marks by the green color, suggest that the area is likely to be more preferable for parking.\n\nimport folium\n\n# Initialize the folium map with light tiles, centered on Downtown LA\nm = folium.Map(location=[34.0407, -118.2468], zoom_start=12, tiles='cartodbpositron')\n\n# Add a Choropleth layer for recommendation scores\nfolium.Choropleth(\n    geo_data=grid,  # GeoDataFrame with grid and recommendation score\n    data=grid,  # Data for the Choropleth\n    columns=['index', 'recommendation_score'],  # Columns to use: 'index' and score\n    key_on='feature.properties.index',  # Match GeoJSON 'index' with data 'index'\n    fill_color='RdYlGn',  # Green for high scores, red for low scores\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    legend_name='Parking Recommendation Score'\n).add_to(m)\n\n# Display the map in the notebook\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Analyzing Parking Spaces in LA"
    ]
  }
]